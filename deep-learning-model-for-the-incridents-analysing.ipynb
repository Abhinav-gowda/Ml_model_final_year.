{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560c837d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-20T14:44:15.186207Z",
     "iopub.status.busy": "2026-01-20T14:44:15.185099Z",
     "iopub.status.idle": "2026-01-20T14:44:16.801971Z",
     "shell.execute_reply": "2026-01-20T14:44:16.800781Z"
    },
    "papermill": {
     "duration": 1.624294,
     "end_time": "2026-01-20T14:44:16.804058",
     "exception": false,
     "start_time": "2026-01-20T14:44:15.179764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of df_ingredients:\n",
      "    Ingredient_Name  Harmfulness_Score  \\\n",
      "0             Water                  1   \n",
      "1          Glycerin                  1   \n",
      "2       Xanthan Gum                  1   \n",
      "3          Carbomer                  1   \n",
      "4  Sodium Gluconate                  1   \n",
      "\n",
      "                                Effect_On_Human_Body  \n",
      "0  Safe solvent used as a base for dissolving oth...  \n",
      "1  Hydrates skin and helps maintain moisture balance  \n",
      "2         Natural thickener; non-toxic and skin-safe  \n",
      "3   Thickening agent; generally safe for topical use  \n",
      "4    Chelating agent that improves product stability  \n",
      "\n",
      "Shape of df_ingredients: (429, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df_ingredients = pd.read_csv('/kaggle/input/ingredient-toxicity-and-health-impact-dataset/ingredient_effects.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First 5 rows of df_ingredients:\")\n",
    "print(df_ingredients.head())\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(f\"\\nShape of df_ingredients: {df_ingredients.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea8a4ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T14:44:16.812089Z",
     "iopub.status.busy": "2026-01-20T14:44:16.811790Z",
     "iopub.status.idle": "2026-01-20T14:44:16.823294Z",
     "shell.execute_reply": "2026-01-20T14:44:16.822309Z"
    },
    "papermill": {
     "duration": 0.018066,
     "end_time": "2026-01-20T14:44:16.825240",
     "exception": false,
     "start_time": "2026-01-20T14:44:16.807174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'Effect_On_Human_Body' sample:\n",
      "['Safe solvent used as a base for dissolving other ingredients', 'Hydrates skin and helps maintain moisture balance', 'Natural thickener; non-toxic and skin-safe', 'Thickening agent; generally safe for topical use', 'Chelating agent that improves product stability']\n",
      "\n",
      "Cleaned 'Effect_On_Human_Body' sample:\n",
      "['safe solvent used as a base for dissolving other ingredients', 'hydrates skin and helps maintain moisture balance', 'natural thickener nontoxic and skinsafe', 'thickening agent generally safe for topical use', 'chelating agent that improves product stability']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Converts text to lowercase, removes punctuation, and strips whitespace.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df_ingredients['cleaned_effects'] = df_ingredients['Effect_On_Human_Body'].apply(clean_text)\n",
    "\n",
    "print(\"Original 'Effect_On_Human_Body' sample:\")\n",
    "print(df_ingredients['Effect_On_Human_Body'].head().tolist())\n",
    "print(\"\\nCleaned 'Effect_On_Human_Body' sample:\")\n",
    "print(df_ingredients['cleaned_effects'].head().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bafa0d41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T14:44:16.832558Z",
     "iopub.status.busy": "2026-01-20T14:44:16.832170Z",
     "iopub.status.idle": "2026-01-20T14:44:41.126676Z",
     "shell.execute_reply": "2026-01-20T14:44:41.125371Z"
    },
    "papermill": {
     "duration": 24.300722,
     "end_time": "2026-01-20T14:44:41.128817",
     "exception": false,
     "start_time": "2026-01-20T14:44:16.828095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 14:44:19.810844: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768920260.172455      17 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768920260.289600      17 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768920261.179479      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768920261.179540      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768920261.179544      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768920261.179546      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 442\n",
      "Sample of encoded documents (first 5):\n",
      "Doc 1: [5, 43, 107, 163, 234, 30, 24, 305, 135, 306]\n",
      "Doc 2: [235, 3, 2, 47, 108, 62, 307]\n",
      "Doc 3: [16, 36, 181, 2, 308]\n",
      "Doc 4: [109, 9, 33, 5, 24, 58, 31]\n",
      "Doc 5: [44, 9, 17, 37, 164, 309]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Instantiate a Tokenizer object. Limiting to top 1000 words for now, can be adjusted.\n",
    "# The 'df_ingredients' variable is available from previous steps.\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<unk>\")\n",
    "\n",
    "# Fit the Tokenizer on the cleaned effects text\n",
    "tokenizer.fit_on_texts(df_ingredients['cleaned_effects'])\n",
    "\n",
    "# Convert text data into sequences of integers\n",
    "encoded_docs = tokenizer.texts_to_sequences(df_ingredients['cleaned_effects'])\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(\"Sample of encoded documents (first 5):\")\n",
    "for i, doc in enumerate(encoded_docs[:5]):\n",
    "    print(f\"Doc {i+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b36c8af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T14:44:41.136749Z",
     "iopub.status.busy": "2026-01-20T14:44:41.135842Z",
     "iopub.status.idle": "2026-01-20T14:44:41.147876Z",
     "shell.execute_reply": "2026-01-20T14:44:41.146603Z"
    },
    "papermill": {
     "duration": 0.018568,
     "end_time": "2026-01-20T14:44:41.150284",
     "exception": false,
     "start_time": "2026-01-20T14:44:41.131716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded sequences: (429, 12)\n",
      "Sample of padded sequences (first 5):\n",
      "Sequence 1: [  5  43 107 163 234  30  24 305 135 306   0   0]\n",
      "Sequence 2: [235   3   2  47 108  62 307   0   0   0   0   0]\n",
      "Sequence 3: [ 16  36 181   2 308   0   0   0   0   0   0   0]\n",
      "Sequence 4: [109   9  33   5  24  58  31   0   0   0   0   0]\n",
      "Sequence 5: [ 44   9  17  37 164 309   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Determine the maximum sequence length (e.g., max length in encoded_docs or a chosen value)\n",
    "# For simplicity, let's find the max length in the current dataset\n",
    "max_sequence_length = max([len(x) for x in encoded_docs])\n",
    "\n",
    "# Pad the sequences to a uniform length\n",
    "padded_sequences = pad_sequences(encoded_docs, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "print(f\"Shape of padded sequences: {padded_sequences.shape}\")\n",
    "print(\"Sample of padded sequences (first 5):\")\n",
    "for i, seq in enumerate(padded_sequences[:5]):\n",
    "    print(f\"Sequence {i+1}: {seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9631314f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T14:44:41.159682Z",
     "iopub.status.busy": "2026-01-20T14:44:41.158990Z",
     "iopub.status.idle": "2026-01-20T14:44:41.421408Z",
     "shell.execute_reply": "2026-01-20T14:44:41.420102Z"
    },
    "papermill": {
     "duration": 0.269702,
     "end_time": "2026-01-20T14:44:41.423448",
     "exception": false,
     "start_time": "2026-01-20T14:44:41.153746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (343, 12)\n",
      "Shape of X_test: (86, 12)\n",
      "Shape of y_train: (343,)\n",
      "Shape of y_test: (86,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = padded_sequences\n",
    "y = df_ingredients['Harmfulness_Score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77da50f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T14:44:41.431763Z",
     "iopub.status.busy": "2026-01-20T14:44:41.430754Z",
     "iopub.status.idle": "2026-01-20T14:44:41.602806Z",
     "shell.execute_reply": "2026-01-20T14:44:41.601723Z"
    },
    "papermill": {
     "duration": 0.178289,
     "end_time": "2026-01-20T14:44:41.604774",
     "exception": false,
     "start_time": "2026-01-20T14:44:41.426485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning Model Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 14:44:41.448202: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">44,300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m44,300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m117,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">161,677</span> (631.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m161,677\u001b[0m (631.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">161,677</span> (631.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m161,677\u001b[0m (631.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
    "\n",
    "# 2. Define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# 3. Define embedding dimension\n",
    "embedding_dim = 100\n",
    "\n",
    "# 4. Define input length (from previous step)\n",
    "input_length = max_sequence_length # This was determined in the padding step\n",
    "\n",
    "# 5. Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Input layer to explicitly define the input shape\n",
    "model.add(Input(shape=(input_length,)))\n",
    "\n",
    "# 6. Add an Embedding layer\n",
    "# input_length is no longer needed here as it's defined by the Input layer\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
    "\n",
    "# 7. Add an LSTM layer, it will now infer its input shape from the Embedding layer\n",
    "model.add(LSTM(units=128)) # Using 128 units as a reasonable starting point\n",
    "\n",
    "# 8. Add a Dense output layer with 1 unit for regression\n",
    "model.add(Dense(units=1, activation=None)) # No activation for regression tasks\n",
    "\n",
    "# 9. Compile the model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# 10. Print the model summary\n",
    "print(\"Deep Learning Model Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b08602bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T14:44:41.614249Z",
     "iopub.status.busy": "2026-01-20T14:44:41.613266Z",
     "iopub.status.idle": "2026-01-20T14:44:48.951523Z",
     "shell.execute_reply": "2026-01-20T14:44:48.950431Z"
    },
    "papermill": {
     "duration": 7.345021,
     "end_time": "2026-01-20T14:44:48.953451",
     "exception": false,
     "start_time": "2026-01-20T14:44:41.608430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the deep learning model with EarlyStopping...\n",
      "Epoch 1/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 20.0020 - val_loss: 5.5588\n",
      "Epoch 2/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.9992 - val_loss: 4.7650\n",
      "Epoch 3/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.0277 - val_loss: 4.2944\n",
      "Epoch 4/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.4583 - val_loss: 3.4531\n",
      "Epoch 5/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.6893 - val_loss: 2.4583\n",
      "Epoch 6/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.5694 - val_loss: 1.3428\n",
      "Epoch 7/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0631 - val_loss: 0.9001\n",
      "Epoch 8/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.7887 - val_loss: 0.8466\n",
      "Epoch 9/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5586 - val_loss: 0.7626\n",
      "Epoch 10/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4985 - val_loss: 0.8929\n",
      "Epoch 11/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3162 - val_loss: 0.7450\n",
      "Epoch 12/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2614 - val_loss: 0.8016\n",
      "Epoch 13/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2464 - val_loss: 0.7422\n",
      "Epoch 14/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1925 - val_loss: 0.7583\n",
      "Epoch 15/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2126 - val_loss: 0.8133\n",
      "Epoch 16/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1772 - val_loss: 0.7976\n",
      "Epoch 17/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1608 - val_loss: 0.8651\n",
      "Epoch 18/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1592 - val_loss: 0.7865\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print(\"Training the deep learning model with EarlyStopping...\")\n",
    "\n",
    "# Instantiate EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', # Monitor validation loss\n",
    "    patience=5,         # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,  # Maximum number of training epochs\n",
    "    batch_size=32, # Batch size for training\n",
    "    validation_data=(X_test, y_test), # Validation data to monitor performance\n",
    "    callbacks=[early_stopping], # Include the EarlyStopping callback\n",
    "    verbose=1 # Show progress bar during training\n",
    ")\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20fa077a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T14:44:48.971426Z",
     "iopub.status.busy": "2026-01-20T14:44:48.970567Z",
     "iopub.status.idle": "2026-01-20T14:44:49.394885Z",
     "shell.execute_reply": "2026-01-20T14:44:49.393652Z"
    },
    "papermill": {
     "duration": 0.435432,
     "end_time": "2026-01-20T14:44:49.397087",
     "exception": false,
     "start_time": "2026-01-20T14:44:48.961655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Predictions on test data generated.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(\"Predictions on test data generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccf86a2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T14:44:49.415714Z",
     "iopub.status.busy": "2026-01-20T14:44:49.414163Z",
     "iopub.status.idle": "2026-01-20T14:44:49.423487Z",
     "shell.execute_reply": "2026-01-20T14:44:49.422308Z"
    },
    "papermill": {
     "duration": 0.021191,
     "end_time": "2026-01-20T14:44:49.426162",
     "exception": false,
     "start_time": "2026-01-20T14:44:49.404971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.7422\n",
      "Root Mean Squared Error (RMSE): 0.8615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce088f63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T14:44:49.444641Z",
     "iopub.status.busy": "2026-01-20T14:44:49.443831Z",
     "iopub.status.idle": "2026-01-20T14:44:49.535197Z",
     "shell.execute_reply": "2026-01-20T14:44:49.534120Z"
    },
    "papermill": {
     "duration": 0.102875,
     "end_time": "2026-01-20T14:44:49.537139",
     "exception": false,
     "start_time": "2026-01-20T14:44:49.434264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Harm Score for 'Causes severe irritation and allergic reactions': 5.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_harm_score(raw_effect_text):\n",
    "    \"\"\"Predicts the harm score for a given raw effect text.\"\"\"\n",
    "    # 1. Clean the raw text\n",
    "    cleaned_text = clean_text(raw_effect_text)\n",
    "\n",
    "    # 2. Tokenize the cleaned text\n",
    "    # texts_to_sequences expects a list of texts, so wrap cleaned_text in a list\n",
    "    encoded_text = tokenizer.texts_to_sequences([cleaned_text])\n",
    "\n",
    "    # 3. Pad the sequence to the maximum length\n",
    "    padded_text = pad_sequences(encoded_text, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    # 4. Predict the harm score using the trained model\n",
    "    predicted_score = model.predict(padded_text, verbose=0)[0][0]\n",
    "\n",
    "    # 5. Clamp the score between 1 and 10\n",
    "    clamped_score = np.clip(predicted_score, 1, 10)\n",
    "\n",
    "    return clamped_score\n",
    "\n",
    "# Test the function with a sample text\n",
    "sample_text = 'Causes severe irritation and allergic reactions'\n",
    "predicted_score_sample = predict_harm_score(sample_text)\n",
    "\n",
    "print(f\"Predicted Harm Score for '{sample_text}': {predicted_score_sample:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cd50a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T14:44:49.554793Z",
     "iopub.status.busy": "2026-01-20T14:44:49.554170Z",
     "iopub.status.idle": "2026-01-20T14:44:49.562863Z",
     "shell.execute_reply": "2026-01-20T14:44:49.561817Z"
    },
    "papermill": {
     "duration": 0.019848,
     "end_time": "2026-01-20T14:44:49.564895",
     "exception": false,
     "start_time": "2026-01-20T14:44:49.545047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Ingredient Harm Scores: [3, 6, 8, 5]\n",
      "Calculated Average Score: 5.50\n",
      "Calculated Maximum Score: 8.00\n",
      "Calculated Final Score: 6.75\n",
      "Product Risk Classification: Moderate Risk\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_product_scores(ingredient_harm_scores):\n",
    "    \"\"\"Calculates average, maximum, and final scores for a product.\"\"\"\n",
    "    if not ingredient_harm_scores:\n",
    "        return 0, 0, 0 # Handle empty list case\n",
    "\n",
    "    average_score = np.mean(ingredient_harm_scores)\n",
    "    maximum_score = np.max(ingredient_harm_scores)\n",
    "    final_score = (average_score + maximum_score) / 2\n",
    "    return average_score, maximum_score, final_score\n",
    "\n",
    "def classify_product_risk(final_score):\n",
    "    \"\"\"Classifies product risk based on the final harm score.\"\"\"\n",
    "    if final_score <= 4:\n",
    "        return 'Safe'\n",
    "    elif final_score <= 7:\n",
    "        return 'Moderate Risk'\n",
    "    else:\n",
    "        return 'Harmful'\n",
    "\n",
    "# Test the functions with a sample list of ingredient harm scores\n",
    "sample_harm_scores = [3, 6, 8, 5]\n",
    "print(f\"Sample Ingredient Harm Scores: {sample_harm_scores}\")\n",
    "\n",
    "average_score, maximum_score, final_score = calculate_product_scores(sample_harm_scores)\n",
    "print(f\"Calculated Average Score: {average_score:.2f}\")\n",
    "print(f\"Calculated Maximum Score: {maximum_score:.2f}\")\n",
    "print(f\"Calculated Final Score: {final_score:.2f}\")\n",
    "\n",
    "product_classification = classify_product_risk(final_score)\n",
    "print(f\"Product Risk Classification: {product_classification}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abdb4144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T14:44:49.583212Z",
     "iopub.status.busy": "2026-01-20T14:44:49.582829Z",
     "iopub.status.idle": "2026-01-20T14:44:49.604982Z",
     "shell.execute_reply": "2026-01-20T14:44:49.603590Z"
    },
    "papermill": {
     "duration": 0.033551,
     "end_time": "2026-01-20T14:44:49.606664",
     "exception": true,
     "start_time": "2026-01-20T14:44:49.573113",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17/960103997.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Get user input for ingredients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0muser_input_ingredients_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter ingredient names separated by commas (e.g., Parabens, Aqua, New Chemical X): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0muser_product_ingredients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mingredient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mingredient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser_input_ingredients_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \"\"\"\n\u001b[1;32m   1173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_stdin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m             raise StdinNotImplementedError(\n\u001b[0m\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "def analyze_product_safety(product_ingredients):\n",
    "    \"\"\"Analyzes a list of product ingredients for safety, predicting scores for unknown ones.\n",
    "    Displays a comprehensive, color-coded safety analysis.\n",
    "    \"\"\"\n",
    "    ingredient_harm_scores = []\n",
    "    ingredient_details = []\n",
    "\n",
    "    # ANSI escape codes for coloring output\n",
    "    COLOR_GREEN = '\\033[92m'\n",
    "    COLOR_YELLOW = '\\033[93m'\n",
    "    COLOR_RED = '\\033[91m'\n",
    "    COLOR_END = '\\033[0m'\n",
    "\n",
    "    print(\"\\n--- Product Safety Analysis ---\")\n",
    "    print(f\"Analyzing product with ingredients: {', '.join(product_ingredients)}\")\n",
    "    print(\"-------------------------------\")\n",
    "\n",
    "    for ingredient_name in product_ingredients:\n",
    "        # Check if the ingredient exists in the dataframe\n",
    "        matching_ingredient = df_ingredients[df_ingredients['Ingredient_Name'].str.lower() == ingredient_name.lower()]\n",
    "\n",
    "        if not matching_ingredient.empty:\n",
    "            # Ingredient found in dataset\n",
    "            harm_score = matching_ingredient['Harmfulness_Score'].iloc[0]\n",
    "            effect_description = matching_ingredient['Effect_On_Human_Body'].iloc[0]\n",
    "            print(f\"  - {ingredient_name}: Known effect - '{effect_description}', Score: {harm_score}\")\n",
    "        else:\n",
    "            # Ingredient not found, predict score\n",
    "            effect_description = \"Unknown compound, potential irritant with cumulative effects. For demonstration purposes.\"\n",
    "            harm_score = predict_harm_score(effect_description)\n",
    "            print(f\"  - {ingredient_name}: *New/Unknown* - Predicted effect: '{effect_description}', Predicted Score: {harm_score:.2f}\")\n",
    "\n",
    "        ingredient_harm_scores.append(harm_score)\n",
    "        ingredient_details.append({\n",
    "            'name': ingredient_name,\n",
    "            'effect': effect_description,\n",
    "            'score': harm_score\n",
    "        })\n",
    "\n",
    "    # Calculate product level scores\n",
    "    average_score, maximum_score, final_score = calculate_product_scores(ingredient_harm_scores)\n",
    "\n",
    "    # Classify product risk\n",
    "    product_classification = classify_product_risk(final_score)\n",
    "\n",
    "    print(\"\\n--- Product Summary ---\")\n",
    "    for detail in ingredient_details:\n",
    "        print(f\"  Ingredient: {detail['name']}\")\n",
    "        print(f\"    Effect: {detail['effect']}\")\n",
    "        print(f\"    Harm Score: {detail['score']:.2f}\")\n",
    "\n",
    "    print(f\"\\n  Average Ingredient Score: {average_score:.2f}\")\n",
    "    print(f\"  Maximum Ingredient Score: {maximum_score:.2f}\")\n",
    "    print(f\"  Final Product Score: {final_score:.2f}\")\n",
    "\n",
    "    # Apply color-coding to the final classification\n",
    "    if product_classification == 'Safe':\n",
    "        color = COLOR_GREEN\n",
    "    elif product_classification == 'Moderate Risk':\n",
    "        color = COLOR_YELLOW\n",
    "    else:\n",
    "        color = COLOR_RED\n",
    "\n",
    "    print(f\"  Product Risk Classification: {color}{product_classification}{COLOR_END}\")\n",
    "    print(\"-------------------------------\")\n",
    "\n",
    "# Get user input for ingredients\n",
    "user_input_ingredients_str = input(\"Enter ingredient names separated by commas (e.g., Parabens, Aqua, New Chemical X): \")\n",
    "user_product_ingredients = [ingredient.strip() for ingredient in user_input_ingredients_str.split(',')]\n",
    "\n",
    "# Analyze the product safety with user-provided ingredients\n",
    "analyze_product_safety(user_product_ingredients)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9239681,
     "sourceId": 14465688,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 41.890649,
   "end_time": "2026-01-20T14:44:53.163010",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-20T14:44:11.272361",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
